% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{openai2024}
{OpenAI}, ``{GPT-4 Technical Report},'' \url{https://platform.openai.com/docs},
  2024, accessed: 2026-01-15.

\bibitem{anthropic2024}
{Anthropic}, ``{Claude 3.5 Model Card},''
  \url{https://www.anthropic.com/claude}, 2024, accessed: 2026-01-15.

\bibitem{google2024}
{Google AI}, ``{Gemini: A Family of Highly Capable Multimodal Models},''
  \url{https://ai.google.dev/gemini-api}, 2024, accessed: 2026-01-15.

\bibitem{gerganov2023}
G.~Gerganov, ``{llama.cpp: Inference of LLaMA model in pure C/C++},''
  \url{https://github.com/ggerganov/llama.cpp}, 2023, accessed: 2026-01-15.

\bibitem{kwon2023}
W.~Kwon, Z.~Li, S.~Zhuang, Y.~Sheng, L.~Zheng, C.~H. Yu, J.~E. Gonzalez,
  H.~Zhang, and I.~Stoica, ``{Efficient Memory Management for Large Language
  Model Serving with PagedAttention},'' in \emph{Proceedings of the ACM SIGOPS
  29th Symposium on Operating Systems Principles}, 2023, pp. 611--626.

\bibitem{nvidia2024}
{NVIDIA}, ``{TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language
  Model Inference},'' \url{https://github.com/NVIDIA/TensorRT-LLM}, 2024,
  accessed: 2026-01-15.

\bibitem{jacob2018}
B.~Jacob, S.~Kligys, B.~Chen, M.~Zhu, M.~Tang, A.~Howard, H.~Adam, and
  D.~Kalenichenko, ``{Quantization and Training of Neural Networks for
  Efficient Integer-Arithmetic-Only Inference},'' in \emph{2018 IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2018, pp. 2704--2713.

\bibitem{dettmers2023}
T.~Dettmers, R.~Svirschevski, V.~Egiazarian, D.~Kuznedelev, E.~Frantar,
  S.~Ashkboos, A.~Borzunov, T.~Hoefler, and D.~Alistarh, ``{SpQR: A
  Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},''
  \emph{arXiv preprint arXiv:2306.03078}, 2023.

\bibitem{frantar2023}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh, ``{GPTQ: Accurate
  Post-Training Quantization for Generative Pre-trained Transformers},''
  \emph{International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{owasp2024}
{OWASP Foundation}, ``{OWASP Application Security Verification Standard (ASVS)
  4.0},''
  \url{https://owasp.org/www-project-application-security-verification-standard/},
  2024, accessed: 2026-01-15.

\bibitem{nist2023}
{National Institute of Standards and Technology}, ``{Artificial Intelligence
  Risk Management Framework (AI RMF 1.0)},'' NIST, Tech. Rep. NIST AI 100-1,
  2023.

\bibitem{iso2023}
{ISO/IEC}, ``{ISO/IEC 42001:2023 Information Technology — Artificial
  Intelligence — Management System},''
  \url{https://www.iso.org/standard/81230.html}, 2023, accessed: 2026-01-15.

\bibitem{redis2024}
{Redis Ltd.}, ``{What is Semantic Caching?}''
  \url{https://redis.io/blog/what-is-semantic-caching/}, 2024, accessed:
  2026-01-15.

\bibitem{scylladb2024}
{ScyllaDB}, ``{Cut LLM Costs and Latency with ScyllaDB Semantic Caching},''
  \url{https://www.scylladb.com/2024/11/24/cut-llm-costs-and-latency-with-scylladb-semantic-caching/},
  2024, accessed: 2026-01-15.

\bibitem{zilliz2023}
{Zilliz}, ``{GPTCache: A Library for Creating Semantic Cache for LLM
  Queries},'' \url{https://github.com/zilliztech/GPTCache}, 2023, accessed:
  2026-01-15.

\bibitem{nvidia2024triton}
{NVIDIA}, ``{Triton Inference Server: Semantic Caching Guide},''
  \url{https://docs.nvidia.com/deeplearning/triton-inference-server/}, 2024,
  accessed: 2026-01-15.

\bibitem{deepseek2025}
{DeepSeek AI}, ``{DeepSeek-R1-Distill-Qwen-1.5B: Technical Specifications},''
  \url{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B}, 2025,
  accessed: 2026-01-15.

\bibitem{sima2025}
{SiMa.ai}, ``{DeepSeek-R1-1.5B on SiMa.ai for Less Than 10 Watts},''
  \url{https://sima.ai/press-release/deepseek-r1-1-5b-on-sima-ai-for-less-than-10-watts/},
  2025, accessed: 2026-01-15.

\end{thebibliography}
