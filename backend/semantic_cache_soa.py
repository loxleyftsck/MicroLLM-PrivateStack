"""
Semantic Cache with SoA (Struct-of-Arrays) Optimization
Optimized embedding storage for 3.5x faster similarity search

Based on benchmark results:
- SoA vs AoS Similarity Search: 3.49x faster
- SoA Partial Dimension Access: 22.26x faster

Author: Herald Michain Samuel Theo Ginting
Project: MicroLLM-PrivateStack
"""

import numpy as np
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
import hashlib
import json
import time


@dataclass
class CacheEntry:
    """Metadata for cached response (stored separately from embeddings)"""
    prompt_hash: str
    prompt: str
    response: Any
    timestamp: float
    hit_count: int = 0


class SemanticCacheSOA:
    """
    Semantic cache using Struct-of-Arrays (SoA) layout for embeddings.
    
    This optimization provides 3.5x faster similarity search compared to
    Array-of-Structs (AoS) layout by improving cache locality.
    
    Architecture:
    - Embeddings stored as (dim, n_entries) matrix for sequential access
    - Metadata stored separately in list
    - SIMD-friendly operations for similarity computation
    - Optional Redis backend for persistence
    """
    
    def __init__(
        self,
        dimension: int = 768,
        max_entries: int = 10000,
        similarity_threshold: float = 0.95,
        embedding_fn=None,
        redis_client=None
    ):
        """
        Initialize semantic cache with SoA storage.
        
        Args:
            dimension: Embedding dimension (default: 768 for sentence-transformers)
            max_entries: Maximum cache entries before eviction
            similarity_threshold: Cosine similarity threshold for cache hit (0.95 = 95%)
            embedding_fn: Function to generate embeddings from text
            redis_client: Optional Redis client for persistence
        """
        self.dimension = dimension
        self.max_entries = max_entries
        self.similarity_threshold = similarity_threshold
        self.embedding_fn = embedding_fn
        self.redis_client = redis_client
        self.redis_key_prefix = "soa_cache:"
        
        # SoA storage: each dimension is a separate array
        # Shape: (dimension, max_entries) for sequential access per dimension
        self.embeddings = np.zeros((dimension, max_entries), dtype=np.float32)
        
        # Precomputed norms for fast similarity calculation
        self.norms = np.zeros(max_entries, dtype=np.float32)
        
        # Metadata storage (separate from embeddings)
        self.entries: List[Optional[CacheEntry]] = [None] * max_entries
        
        # Index management
        self.n_entries = 0
        self.total_hits = 0
        self.total_misses = 0
        
        # Load from Redis if available
        if self.redis_client:
            self._load_from_redis()
        
        backend = "Redis" if self.redis_client else "In-Memory"
        print(f"âœ… SemanticCacheSOA initialized: {dimension}D Ã— {max_entries} entries ({backend})")
        print(f"   Memory: {self.embeddings.nbytes / 1024**2:.2f} MB embeddings")
    
    def _generate_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for text using provided function or random fallback"""
        if self.embedding_fn:
            return self.embedding_fn(text)
        else:
            # Fallback: hash-based pseudo-embedding (for testing)
            hash_bytes = hashlib.sha256(text.encode()).digest()
            # Expand hash to dimension size
            np.random.seed(int.from_bytes(hash_bytes[:4], 'big'))
            return np.random.randn(self.dimension).astype(np.float32)
    
    def _compute_similarities(self, query_embedding: np.ndarray) -> np.ndarray:
        """
        Compute cosine similarities using SoA layout for optimal cache performance.
        
        This is the core optimization: sequential access to each dimension array.
        """
        if self.n_entries == 0:
            return np.array([])
        
        # Get active embeddings slice
        active_embeddings = self.embeddings[:, :self.n_entries]  # (dim, n_entries)
        active_norms = self.norms[:self.n_entries]
        
        # Vectorized dot product (query @ embeddings.T equivalent, but SoA-optimized)
        # Shape: (n_entries,)
        dots = np.dot(query_embedding, active_embeddings)
        
        # Normalize by query norm and cached norms
        query_norm = np.linalg.norm(query_embedding)
        
        # Avoid division by zero
        valid_mask = (active_norms > 1e-8) & (query_norm > 1e-8)
        similarities = np.zeros(self.n_entries, dtype=np.float32)
        similarities[valid_mask] = dots[valid_mask] / (query_norm * active_norms[valid_mask])
        
        return similarities
    
    def get(self, prompt: str, **kwargs) -> Tuple[Optional[Any], float]:
        """
        Get cached response using semantic similarity.
        
        Args:
            prompt: User prompt to look up
            **kwargs: Additional parameters (for future use)
            
        Returns:
            Tuple of (response or None, similarity_score)
        """
        start_time = time.perf_counter()
        
        if self.n_entries == 0:
            self.total_misses += 1
            return None, 0.0
        
        # Generate query embedding
        query_embedding = self._generate_embedding(prompt)
        
        # Compute similarities using SoA-optimized method
        similarities = self._compute_similarities(query_embedding)
        
        # Find best match
        best_idx = np.argmax(similarities)
        best_similarity = similarities[best_idx]
        
        lookup_time = (time.perf_counter() - start_time) * 1000
        
        if best_similarity >= self.similarity_threshold:
            # Cache HIT
            entry = self.entries[best_idx]
            if entry:
                entry.hit_count += 1
                self.total_hits += 1
                print(f"ðŸŽ¯ Semantic Cache HIT: similarity={best_similarity:.3f}, time={lookup_time:.2f}ms")
                return entry.response, best_similarity
        
        # Cache MISS
        self.total_misses += 1
        return None, best_similarity
    
    def set(self, prompt: str, response: Any, **kwargs) -> int:
        """
        Cache a response with its embedding.
        
        Args:
            prompt: User prompt
            response: Response to cache
            **kwargs: Additional parameters
            
        Returns:
            Index where entry was stored
        """
        # Generate embedding
        embedding = self._generate_embedding(prompt)
        
        # Find storage index (evict oldest if full)
        if self.n_entries >= self.max_entries:
            idx = self._find_eviction_candidate()
        else:
            idx = self.n_entries
            self.n_entries += 1
        
        # Store embedding in SoA layout (column-wise write)
        self.embeddings[:, idx] = embedding
        self.norms[idx] = np.linalg.norm(embedding)
        
        # Store metadata
        self.entries[idx] = CacheEntry(
            prompt_hash=hashlib.sha256(prompt.encode()).hexdigest()[:16],
            prompt=prompt[:200],  # Truncate for storage
            response=response,
            timestamp=time.time()
        )
        
        # Save to Redis if available
        if self.redis_client:
            self._save_to_redis(idx)
        
        print(f"ðŸ’¾ Semantic Cache SET: idx={idx}, entries={self.n_entries}")
        return idx
    
    def _find_eviction_candidate(self) -> int:
        """Find entry to evict using LRU-like policy weighted by hit count"""
        oldest_idx = 0
        oldest_score = float('inf')
        
        for i in range(self.n_entries):
            entry = self.entries[i]
            if entry:
                # Score: lower is better candidate for eviction
                # Prefer evicting old entries with low hit counts
                score = entry.timestamp + (entry.hit_count * 3600)  # 1 hit = 1 hour protection
                if score < oldest_score:
                    oldest_score = score
                    oldest_idx = i
        
        return oldest_idx
    
    def invalidate(self, prompt: Optional[str] = None) -> int:
        """
        Invalidate cache entries.
        
        Args:
            prompt: Specific prompt to invalidate, or None for all
            
        Returns:
            Number of entries invalidated
        """
        if prompt is None:
            # Clear all
            count = self.n_entries
            self.embeddings[:] = 0
            self.norms[:] = 0
            self.entries = [None] * self.max_entries
            self.n_entries = 0
            print(f"ðŸ—‘ï¸ Invalidated all {count} cache entries")
            return count
        
        # Find and remove specific entry
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]
        count = 0
        
        for i in range(self.n_entries):
            entry = self.entries[i]
            if entry and entry.prompt_hash == prompt_hash:
                self.entries[i] = None
                self.embeddings[:, i] = 0
                self.norms[i] = 0
                count += 1
        
        return count
    
    def _load_from_redis(self):
        """Load cache from Redis backend"""
        if not self.redis_client:
            return
        
        try:
            # Get count of entries
            count_key = f"{self.redis_key_prefix}count"
            resp = self.redis_client.get(count_key)
            if resp:
                self.n_entries = int(resp.decode('utf-8'))
                
            # Load embeddings binary blob (DO NOT DECODE - its binary)
            emb_key = f"{self.redis_key_prefix}embeddings"
            emb_data = self.redis_client.get(emb_key)
            if emb_data:
                # Assuming raw bytes from numpy.tobytes()
                loaded_emb = np.frombuffer(emb_data, dtype=np.float32)
                expected_size = self.dimension * self.max_entries
                if len(loaded_emb) == expected_size:
                    self.embeddings = loaded_emb.reshape((self.dimension, self.max_entries))
                    self.norms = np.linalg.norm(self.embeddings, axis=0)
            
            # Load metadata entries
            for i in range(self.n_entries):
                entry_key = f"{self.redis_key_prefix}entry:{i}"
                entry_data = self.redis_client.get(entry_key)
                if entry_data:
                    data = json.loads(entry_data.decode('utf-8'))
                    self.entries[i] = CacheEntry(**data)
            
            print(f"âœ… Loaded {self.n_entries} entries from Redis")
        except Exception as e:
            print(f"âš ï¸ Failed to load from Redis: {e}")
    
    def _save_to_redis(self, idx: int):
        """Save cache state to Redis"""
        if not self.redis_client:
            return
        
        try:
            # Save metadata entry
            entry = self.entries[idx]
            if entry:
                entry_data = {
                    'prompt_hash': entry.prompt_hash,
                    'prompt': entry.prompt,
                    'response': entry.response,
                    'timestamp': entry.timestamp,
                    'hit_count': entry.hit_count
                }
                entry_key = f"{self.redis_key_prefix}entry:{idx}"
                self.redis_client.set(entry_key, json.dumps(entry_data))
                
            # Save count
            count_key = f"{self.redis_key_prefix}count"
            self.redis_client.set(count_key, self.n_entries)
            
            # Save whole embeddings blob (for reliability on restart)
            # Potentially large, but for 10000 entries of 768 float32 it's ~30MB
            emb_key = f"{self.redis_key_prefix}embeddings"
            self.redis_client.set(emb_key, self.embeddings.tobytes())
            
        except Exception as e:
            print(f"âš ï¸ Failed to save to Redis: {e}")
    
    def stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.total_hits + self.total_misses
        hit_rate = (self.total_hits / total_requests * 100) if total_requests > 0 else 0
        
        # Memory usage
        embedding_memory_mb = self.embeddings.nbytes / 1024**2
        
        return {
            'enabled': True,
            'storage_type': 'SoA (Struct-of-Arrays)',
            'dimension': self.dimension,
            'max_entries': self.max_entries,
            'current_entries': self.n_entries,
            'similarity_threshold': self.similarity_threshold,
            'total_hits': self.total_hits,
            'total_misses': self.total_misses,
            'hit_rate_pct': round(hit_rate, 2),
            'embedding_memory_mb': round(embedding_memory_mb, 2)
        }
    
    def benchmark_lookup(self, n_queries: int = 100) -> Dict[str, float]:
        """Benchmark lookup performance"""
        if self.n_entries == 0:
            return {'error': 'Cache is empty'}
        
        times = []
        for _ in range(n_queries):
            # Generate random query
            query = f"test query {np.random.randint(0, 10000)}"
            start = time.perf_counter()
            _ = self.get(query)
            end = time.perf_counter()
            times.append((end - start) * 1000)
        
        return {
            'n_queries': n_queries,
            'mean_ms': round(np.mean(times), 3),
            'median_ms': round(np.median(times), 3),
            'p95_ms': round(np.percentile(times, 95), 3),
            'p99_ms': round(np.percentile(times, 99), 3)
        }


class SemanticCacheAoS:
    """
    Legacy Array-of-Structs implementation for comparison.
    Use SemanticCacheSOA for production - it's 3.5x faster!
    """
    
    def __init__(self, dimension: int = 768, max_entries: int = 10000, 
                 similarity_threshold: float = 0.95, embedding_fn=None):
        self.dimension = dimension
        self.max_entries = max_entries
        self.similarity_threshold = similarity_threshold
        self.embedding_fn = embedding_fn
        
        # AoS storage: list of (embedding, metadata) tuples
        self.cache: List[Tuple[np.ndarray, CacheEntry]] = []
        
        self.total_hits = 0
        self.total_misses = 0
        
        print(f"âš ï¸ SemanticCacheAoS (legacy) initialized - consider using SemanticCacheSOA")
    
    def _generate_embedding(self, text: str) -> np.ndarray:
        if self.embedding_fn:
            return self.embedding_fn(text)
        else:
            hash_bytes = hashlib.sha256(text.encode()).digest()
            np.random.seed(int.from_bytes(hash_bytes[:4], 'big'))
            return np.random.randn(self.dimension).astype(np.float32)
    
    def get(self, prompt: str, **kwargs) -> Tuple[Optional[Any], float]:
        if not self.cache:
            self.total_misses += 1
            return None, 0.0
        
        query_embedding = self._generate_embedding(prompt)
        query_norm = np.linalg.norm(query_embedding)
        
        best_similarity = 0.0
        best_entry = None
        
        # AoS: iterate through list of structs
        for embedding, entry in self.cache:
            similarity = np.dot(query_embedding, embedding) / (query_norm * np.linalg.norm(embedding))
            if similarity > best_similarity:
                best_similarity = similarity
                best_entry = entry
        
        if best_similarity >= self.similarity_threshold and best_entry:
            self.total_hits += 1
            return best_entry.response, best_similarity
        
        self.total_misses += 1
        return None, best_similarity
    
    def set(self, prompt: str, response: Any, **kwargs) -> int:
        embedding = self._generate_embedding(prompt)
        
        entry = CacheEntry(
            prompt_hash=hashlib.sha256(prompt.encode()).hexdigest()[:16],
            prompt=prompt[:200],
            response=response,
            timestamp=time.time()
        )
        
        if len(self.cache) >= self.max_entries:
            self.cache.pop(0)  # Simple FIFO eviction
        
        self.cache.append((embedding, entry))
        return len(self.cache) - 1


# Convenience function to choose best implementation
def create_semantic_cache(
    dimension: int = 768,
    max_entries: int = 10000,
    similarity_threshold: float = 0.95,
    embedding_fn=None,
    redis_client=None,
    use_soa: bool = True
):
    """
    Factory function to create semantic cache.
    
    Args:
        use_soa: Use SoA optimization (True) or legacy AoS (False)
        redis_client: Optional Redis client for persistence (SoA only)
    
    Returns:
        SemanticCacheSOA or SemanticCacheAoS instance
    """
    if use_soa:
        return SemanticCacheSOA(dimension, max_entries, similarity_threshold, embedding_fn, redis_client)
    else:
        return SemanticCacheAoS(dimension, max_entries, similarity_threshold, embedding_fn)


if __name__ == "__main__":
    print("=" * 60)
    print("SEMANTIC CACHE SOA DEMONSTRATION")
    print("=" * 60)
    
    # Create cache
    cache = SemanticCacheSOA(dimension=768, max_entries=1000)
    
    # Populate with test data
    print("\nPopulating cache with 100 entries...")
    for i in range(100):
        cache.set(f"What is machine learning concept {i}?", f"Response {i}")
    
    # Test lookup
    print("\nTesting lookups...")
    response, similarity = cache.get("What is machine learning concept 50?")
    print(f"  Exact match: similarity={similarity:.3f}")
    
    response, similarity = cache.get("What is ML concept fifty?")
    print(f"  Similar query: similarity={similarity:.3f}")
    
    # Benchmark
    print("\nRunning benchmark...")
    bench_results = cache.benchmark_lookup(n_queries=100)
    print(f"  Lookup performance: {bench_results}")
    
    # Stats
    print("\nCache statistics:")
    stats = cache.stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
