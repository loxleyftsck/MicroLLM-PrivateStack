# MicroLLM-PrivateStack Optimal Configuration
# Generated from trade-off analysis on 2026-01-18

# ============================================
# LLM Engine Settings (OPTIMIZED)
# ============================================

# Model path
MODEL_PATH=./models/deepseek-r1-1.5b-q4.gguf

# Context and generation settings
MODEL_CONTEXT_LENGTH=512
MODEL_THREADS=2
MODEL_BATCH=256

# OPTIMAL TRADE-OFF SETTINGS
# Based on benchmark scoring:
# - Speed: 100/100
# - Quality: 82.4/100
# - Efficiency: 100/100
# - Reliability: 100/100
# - TOTAL: 94.7/100
MODEL_TEMPERATURE=0.7
MODEL_TOP_P=0.9

# Default max_tokens for API (can be overridden per request)
# 128 provides best balance between response quality and speed
DEFAULT_MAX_TOKENS=128

# ============================================
# Cache Settings
# ============================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_ENABLED=true
CACHE_TTL=3600

# Semantic cache similarity threshold
# 0.95 = Only return cached response if query is >95% similar
CACHE_SIMILARITY_THRESHOLD=0.95

# ============================================
# Security Settings
# ============================================
JWT_SECRET_KEY=change-me-in-production
STRICT_MODE=true
TOXICITY_THRESHOLD=0.7
HALLUCINATION_THRESHOLD=0.8
MASK_PII=true

# ============================================
# Server Settings
# ============================================
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=false
LOG_LEVEL=INFO
